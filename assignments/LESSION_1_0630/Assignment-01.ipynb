{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson-01 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`各位同学大家好，欢迎各位开始学习我们的人工智能课程。这门课程假设大家不具备机器学习和人工智能的知识，但是希望大家具备初级的Python编程能力。根据往期同学的实际反馈，我们课程的完结之后 能力能够超过80%的计算机人工智能/深度学习方向的硕士生的能力。`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本次作业的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 复现课堂代码\n",
    "\n",
    "在本部分，你需要参照我们给大家的GitHub地址里边的课堂代码，结合课堂内容，复现内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 请回答以下问题\n",
    "\n",
    "回答以下问题，并将问题发送至 mqgao@kaikeba.com中：\n",
    "```\n",
    "    2.1. what do you want to acquire in this course？\n",
    "    2.2. what problems do you want to solve？\n",
    "    2.3. what’s the advantages you have to finish you goal?\n",
    "    2.4. what’s the disadvantages you need to overcome to finish you goal?\n",
    "    2.5. How will you plan to study in this course period?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 如何提交\n",
    "代码 + 此 jupyter 相关，提交至自己的 github 中(**所以请务必把GitHub按照班主任要求录入在Trello中**)；\n",
    "第2问，请提交至mqgao@kaikeba.com邮箱。\n",
    "#### 4. 作业截止时间\n",
    "此次作业截止时间为 2019.7.6日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 完成以下问答和编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础理论部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Can you come up out 3 sceneraies which use AI methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: {智能音响,无人驾驶,物流无人货仓}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How do we use Github; Why do we use Jupyter and Pycharm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: {github用来记录、更新代码，jupyter用来单步调试代码，pycharm编写调试完整程序}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:用概率统计方法建立的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Can you came up with some sceneraies at which we could use Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:区分垃圾邮件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:拿垃圾邮件来说，传统的单词匹配方法得出的正确率达不到要求，而且随着输入变化包含不了所有情况，我们需要使用概率统计模型；难点在于如何正确快速地分析样本，样本分析的效率问题，如何能在输入变化的情况下让分析过程代码不需要变化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What's the Language Model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:参考课上和维基百科-语言模型，统计语言模型是一个单词序列上的概率分布，想办法找到一个概率分布，可以表示任意一个句子或者序列出现的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you came up with some sceneraies at which we could use Language Model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:语音识别，机器人对话"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. What's the 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "结合课上课后我的理解：n元文法是给定一个样本，然后根据样本计算出某个句子出现的可能性；\n",
    "1-gram 即一元文法模型是一种上下文无关的模型，仅仅考虑当前词本身出现的概率，那么一个句子出现的概率等于每个单词出现概率的乘积，可以理解为课上讲的 prob_1(word) 结果相乘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. What's the disadvantages and advantages of 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:优点：计算方便，仅仅计算单个词在整个样本中出现的概率；缺点：正确性不高，因为它缺少上下文关系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. What't the 2-gram models;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:二元文法是一种上下文有关的模型，我们认为当前词只与它前面的一个词相关，它计算出一句话出现的概率为前后两两相关的词出现概率的乘积，可以理解为课上讲的 prob_2(word1, word2) 结果相乘\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编程实践部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 设计你自己的句子生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何生成句子是一个很经典的问题，从1940s开始，图灵提出机器智能的时候，就使用的是人类能不能流畅和计算机进行对话。和计算机对话的一个前提是，计算机能够生成语言。\n",
    "\n",
    "计算机如何能生成语言是一个经典但是又很复杂的问题。 我们课程上为大家介绍的是一种基于规则（Rule Based）的生成方法。该方法虽然提出的时间早，但是现在依然在很多地方能够大显身手。值得说明的是，现在很多很实用的算法，都是很久之前提出的，例如，二分查找提出与1940s, Dijstra算法提出于1960s 等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在著名的电视剧，电影《西部世界》中，这些机器人们语言生成的方法就是使用的SyntaxTree生成语言的方法。\n",
    "\n",
    "> \n",
    ">\n",
    "\n",
    "![WstWorld](https://timgsa.baidu.com/timg?image&quality=80&size=b10000_10000&sec=1561818705&di=95ca9ff2ff37fcb88ae47b82c7079feb&src=http://s7.sinaimg.cn/mw690/006BKUGwzy75VK46FMi66&690)\n",
    "\n",
    "> \n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这一部分，需要各位同学首先定义自己的语言。 大家可以先想一个应用场景，然后在这个场景下，定义语法。例如：\n",
    "\n",
    "在西部世界里，一个”人类“的语言可以定义为：\n",
    "``` \n",
    "human = \"\"\"\n",
    "human = 自己 寻找 活动\n",
    "自己 = 我 | 俺 | 我们 \n",
    "寻找 = 看看 | 找找 | 想找点\n",
    "活动 = 乐子 | 玩的\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "一个“接待员”的语言可以定义为\n",
    "```\n",
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = 耍一耍 | 玩一玩\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请定义你自己的语法: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': [['时间', '人物', '事情']],\n",
       " '时间': [['今天'], ['明天'], ['周末']],\n",
       " '人物': [['和朋友'], ['和李磊'], ['独自']],\n",
       " '事情': [['看战狼'], ['吃大餐'], ['上课'], ['旅游']]}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xiaoming = '''\n",
    "sentence = 时间 人物 事情\n",
    "时间 = 今天 | 明天 | 周末\n",
    "人物 = 和朋友 | 和李磊 | 独自\n",
    "事情 = 看战狼 | 吃大餐 | 上课 | 旅游\n",
    "'''\n",
    "def create_grammar(grammar_str, split = '=', line_split = '\\n'):\n",
    "    grammar = {}\n",
    "    for line in grammar_str.split(line_split):\n",
    "        if line.strip():\n",
    "            key, value = line.split(split)\n",
    "            grammar[key.strip()] = [a.split() for a in value.split('|')]\n",
    "    return grammar\n",
    "xiaoming_grammar = create_grammar(xiaoming)\n",
    "xiaoming_grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': [['时间', '判断词', '状态']],\n",
       " '时间': [['本周'], ['后续几天'], ['今明两天']],\n",
       " '判断词': [['将会'], ['肯定'], ['不会'], ['很大可能会']],\n",
       " '状态': [['下大雨'], ['下冰雹'], ['下雪'], ['是一个晴天']]}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tianqi = '''\n",
    "sentence = 时间 判断词 状态\n",
    "时间 = 本周 | 后续几天 | 今明两天\n",
    "判断词 = 将会 | 肯定 | 不会 | 很大可能会\n",
    "状态 = 下大雨 | 下冰雹 | 下雪 | 是一个晴天\n",
    "'''\n",
    "tianqi_grammar = create_grammar(tianqi)\n",
    "tianqi_grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，使用自己之前定义的generate函数，使用此函数生成句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'明天独自上课'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "def generate(gram, target):\n",
    "    if target not in gram:\n",
    "        return target\n",
    "    expand = [generate(gram, t) for t in random.choice(gram[target])]\n",
    "    return ''.join(expand)\n",
    "generate(xiaoming_grammar, 'sentence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，定义一个函数，generate_n，将generate扩展，使其能够生成n个句子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['明天独自旅游', '明天和李磊旅游', '明天和李磊看战狼', '周末和朋友看战狼']"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_n(n, gram):\n",
    "    sen = []\n",
    "    for i in range(n):\n",
    "        sen.append(generate(create_grammar(gram), target = 'sentence'))\n",
    "    return sen\n",
    "generate_n(4, xiaoming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 使用新数据源完成语言模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照我们上文中定义的`prob_2`函数，我们更换一个文本数据源，获得新的Language Model:\n",
    "\n",
    "1. 下载文本数据集（你可以在以下数据集中任选一个，也可以两个都使用）\n",
    "    + 可选数据集1，保险行业问询对话集： https://github.com/Computing-Intelligence/insuranceqa-corpus-zh/raw/release/corpus/pool/train.txt.gz\n",
    "    + 可选数据集2：豆瓣评论数据集：https://github.com/Computing-Intelligence/datasource/raw/master/movie_comments.csv\n",
    "2. 修改代码，获得新的**2-gram**语言模型\n",
    "    + 进行文本清洗，获得所有的纯文本\n",
    "    + 将这些文本进行切词\n",
    "    + 送入之前定义的语言模型中，判断文本的合理程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用豆瓣评论数据集\n",
    "import jieba, re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "#进行文本清洗，获得所有的纯文本\n",
    "file_name = 'movie_comments.csv'\n",
    "content = pd.read_csv(file_name, encoding = 'utf-8')\n",
    "comments = content['comment'].tolist()\n",
    "\n",
    "def find_words(string):\n",
    "    return re.findall('\\w+', string)\n",
    "\n",
    "comments_clean = [''.join(find_words(str(a))) for a in comments]\n",
    "with open('clean_comments.txt', 'w', encoding = 'utf-8') as f:\n",
    "    for a in comments_clean:\n",
    "        f.write(a + '\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50000\n",
      "100000\n",
      "150000\n",
      "200000\n",
      "250000\n"
     ]
    }
   ],
   "source": [
    "#将这些文本进行切词\n",
    "TOKEN = []\n",
    "for i, line in enumerate((open('clean_comments.txt', 'r', encoding = 'utf-8'))):\n",
    "    line = re.sub(r'\\n', '', line)\n",
    "    if i % 50000 == 0:\n",
    "        print(i)\n",
    "    TOKEN += list(jieba.cut(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "#送入之前定义的语言模型中，判断文本的合理程度\n",
    "#print(TOKEN[:10])\n",
    "words_count = Counter(TOKEN)\n",
    "\n",
    "TOKEN = [str(t) for t in TOKEN]\n",
    "TOKEN_2_GRAM = [''.join(TOKEN[i:i + 2]) for i in range(len(TOKEN[:-1]))]\n",
    "\n",
    "words_count_2 = Counter(TOKEN_2_GRAM)\n",
    "\n",
    "#words_count.most_common(10)\n",
    "def prob_1(word):\n",
    "    return words_count[word] / len(TOKEN)\n",
    "\n",
    "def prob_2(word1, word2):\n",
    "    if word1 + word2 in words_count_2:\n",
    "        return words_count_2[word1 + word2] / words_count[word1]\n",
    "    else:\n",
    "        return 1 / len(TOKEN_2_GRAM)\n",
    "prob_2('动作', '场面')\n",
    "\n",
    "def get_probability(sentence):\n",
    "    words = list(jieba.cut(sentence))\n",
    "    sentence_pro = 1\n",
    "    #print(list(words))\n",
    "    for i, word in enumerate(words[:-1]):\n",
    "        next_ = words[i + 1]\n",
    "        if i == 0:\n",
    "            probability = prob_1(word)\n",
    "            probability *= prob_2(word, next_)\n",
    "        else:\n",
    "            probability = prob_2(word, next_)\n",
    "        sentence_pro *= probability\n",
    "    return sentence_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6591187596032596e-16\n",
      "7.500369951949566e-15\n",
      "1.6956683388482733e-08\n",
      "1.8676977940659695e-30\n"
     ]
    }
   ],
   "source": [
    "print(get_probability('吴京的电影是战狼2'))\n",
    "print(get_probability('我看过战狼2的电影'))\n",
    "print(get_probability('今天天气很好'))\n",
    "print(get_probability('不我鬼你什通是话'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 获得最优质的的语言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们能够生成随机的语言并且能判断之后，我们就可以生成更加合理的语言了。请定义 generate_best 函数，该函数输入一个语法 + 语言模型，能够生成**n**个句子，并能选择一个最合理的句子: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示，要实现这个函数，你需要Python的sorted函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 5]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([1, 3, 5, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数接受一个参数key，这个参数接受一个函数作为输入，例如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (2, 5), (4, 4), (5, 0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第0个元素进行排序."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0), (1, 4), (4, 4), (2, 5)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第1个元素进行排序."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 5), (1, 4), (4, 4), (5, 0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第1个元素进行排序, 但是是递减的顺序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('今天我和朋友在餐厅元许很高兴', 3.851988022497157e-33)\n",
      "('今天我非弹在电影院吃大餐吃奶昔', 3.408249808412723e-44)\n",
      "('今天我和机器人在餐厅元许抄2抄', 7.349642151731362e-45)\n",
      "('明天搜能和机器人磊里元许很高兴', 7.238886671391888e-45)\n",
      "('明天我非弹磊里旅率抄2抄', 5.895545702991925e-45)\n",
      "('末周京巴和机器人在餐厅旅率很高兴', 2.911152323240682e-45)\n",
      "('今天人超非弹在电影院元许吃奶昔', 3.637897940588392e-46)\n",
      "('明天人超和朋友在餐厅吃大餐很高兴', 3.536752390076582e-46)\n",
      "('末周人超和朋友磊里吃大餐吃奶昔', 2.3135843976873847e-51)\n",
      "('今天搜能和朋友在餐厅元许抄2抄', 1.1455952512878861e-52)\n",
      "('今天搜能非弹在电影院元许抄2抄', 8.101659618726698e-53)\n",
      "('明天京巴和机器人磊里旅率抄2抄', 4.61566854618333e-55)\n",
      "('末周人超查扯来在餐厅元许很高兴', 3.1436100722728355e-56)\n",
      "('今天京巴和朋友磊里吃大餐抄2抄', 9.515479894780235e-57)\n",
      "('末周京巴查扯来在餐厅看战狼2抄2抄', 5.717341978661608e-68)\n",
      "('顺丫递京巴和机器人在餐厅旅率很高兴', 0.0)\n",
      "('顺丫递人超和朋友在电影院看战狼2啦锅', 0.0)\n",
      "('末周搜能非弹机苹看战狼2抄2抄', 0.0)\n",
      "('顺丫递搜能查扯来磊里看战狼2吃奶昔', 0.0)\n",
      "('顺丫递我查扯来机苹元许很高兴', 0.0)\n",
      "('顺丫递搜能查扯来磊里元许抄2抄', 0.0)\n",
      "('顺丫递京巴查扯来在电影院旅率很高兴', 0.0)\n",
      "('末周我查扯来在餐厅元许啦锅', 0.0)\n",
      "('顺丫递人超查扯来在电影院看战狼2啦锅', 0.0)\n",
      "('顺丫递搜能和朋友在餐厅旅率吃奶昔', 0.0)\n",
      "('顺丫递人超非弹磊里吃大餐吃奶昔', 0.0)\n",
      "('顺丫递京巴查扯来在电影院吃大餐抄2抄', 0.0)\n",
      "('顺丫递京巴查扯来机苹看战狼2抄2抄', 0.0)\n",
      "('顺丫递搜能和朋友在电影院元许吃奶昔', 0.0)\n",
      "('顺丫递搜能和朋友机苹看战狼2很高兴', 0.0)\n",
      "most_human_sentence:  今天我和朋友在餐厅元许很高兴\n"
     ]
    }
   ],
   "source": [
    "word_collected = '''\n",
    "sentence = 时间 主语 人物 地点 动作 尾声  \n",
    "时间 = 今天 | 明天 | 末周 | 顺丫递\n",
    "主语 = 我 | 京巴 | 人超 | 搜能\n",
    "人物 = 和朋友 | 和机器人 | 非弹  | 查扯来 \n",
    "地点 = 在电影院| 在餐厅 | 磊里 | 机苹 \n",
    "动作 = 看战狼2 | 吃大餐 | 元许 | 旅率\n",
    "尾声 = 很高兴 | 吃奶昔 | 抄2抄 | 啦锅\n",
    "'''\n",
    "\n",
    "import math\n",
    "\n",
    "def generate_best(n, gram): \n",
    "    all_sentence = generate_n(n, gram)\n",
    "    list = []\n",
    "    for i, sentence in enumerate(all_sentence):\n",
    "        list.append((sentence, get_probability(sentence)))\n",
    "    sorted_list = sorted(list, key = lambda x: x[1], reverse=True)\n",
    "    for i in sorted_list:\n",
    "        print(i)\n",
    "    return sorted_list[0][0]\n",
    "    \n",
    "most_human_sentence = generate_best(30, word_collected)\n",
    "print('most_human_sentence: ', most_human_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了，现在我们实现了自己的第一个AI模型，这个模型能够生成比较接近于人类的语言。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 这个模型有什么问题？ 你准备如何提升？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "某次的结果：\n",
    "('今天我和朋友在餐厅元许很高兴', 3.851988022497157e-33)\n",
    "('今天我非弹在电影院吃大餐吃奶昔', 3.408249808412723e-44)\n",
    "('今天我和机器人在餐厅元许抄2抄', 7.349642151731362e-45)\n",
    "('明天搜能和机器人磊里元许很高兴', 7.238886671391888e-45)\n",
    "('明天我非弹磊里旅率抄2抄', 5.895545702991925e-45)\n",
    "('末周京巴和机器人在餐厅旅率很高兴', 2.911152323240682e-45)\n",
    "('今天人超非弹在电影院元许吃奶昔', 3.637897940588392e-46)\n",
    "('明天人超和朋友在餐厅吃大餐很高兴', 3.536752390076582e-46)\n",
    "('末周人超和朋友磊里吃大餐吃奶昔', 2.3135843976873847e-51)\n",
    "('今天搜能和朋友在餐厅元许抄2抄', 1.1455952512878861e-52)\n",
    "('今天搜能非弹在电影院元许抄2抄', 8.101659618726698e-53)\n",
    "('明天京巴和机器人磊里旅率抄2抄', 4.61566854618333e-55)\n",
    "('末周人超查扯来在餐厅元许很高兴', 3.1436100722728355e-56)\n",
    "('今天京巴和朋友磊里吃大餐抄2抄', 9.515479894780235e-57)\n",
    "('末周京巴查扯来在餐厅看战狼2抄2抄', 5.717341978661608e-68)\n",
    "('顺丫递京巴和机器人在餐厅旅率很高兴', 0.0)\n",
    "('顺丫递人超和朋友在电影院看战狼2啦锅', 0.0)\n",
    "('末周搜能非弹机苹看战狼2抄2抄', 0.0)\n",
    "('顺丫递搜能查扯来磊里看战狼2吃奶昔', 0.0)\n",
    "('顺丫递我查扯来机苹元许很高兴', 0.0)\n",
    "('顺丫递搜能查扯来磊里元许抄2抄', 0.0)\n",
    "('顺丫递京巴查扯来在电影院旅率很高兴', 0.0)\n",
    "('末周我查扯来在餐厅元许啦锅', 0.0)\n",
    "('顺丫递人超查扯来在电影院看战狼2啦锅', 0.0)\n",
    "('顺丫递搜能和朋友在餐厅旅率吃奶昔', 0.0)\n",
    "('顺丫递人超非弹磊里吃大餐吃奶昔', 0.0)\n",
    "('顺丫递京巴查扯来在电影院吃大餐抄2抄', 0.0)\n",
    "('顺丫递京巴查扯来机苹看战狼2抄2抄', 0.0)\n",
    "('顺丫递搜能和朋友在电影院元许吃奶昔', 0.0)\n",
    "('顺丫递搜能和朋友机苹看战狼2很高兴', 0.0)\n",
    "most_human_sentence:  今天我和朋友在餐厅元许很高兴\n",
    "\n",
    "我定义的word_collected  语法选词是有规律的：前两个词有意义， 后两个词没意义；\n",
    "比如 \"时间 = 今天 | 明天 | 末周 | 顺丫递\" 今天和明天是有意义的，末周和顺丫递没有意义。\n",
    "\n",
    "那通过这个模型选出来的最近似的语句:'今天我和朋友在餐厅元许很高兴',其实选出了 5/6 的有意义的单词，只有 '元许'没有意义，也反向证明了这个模型的正确性，如果想全部选出有意义的单词，可以增加  generate_best(n, gram) 中的n来获得更多样本。\n",
    "\n",
    "\n",
    "这个模型的问题：\n",
    "1.基于豆瓣的评论样本，并不能全部涵盖语法中出现的单词，如果语法跟电影无关，算出来的概率就没有太大的参考意义。\n",
    "2.即使选出单个对人类有意义的单词，并不能构造出整句有意义的语句，比如'明天京巴和机器人在餐厅看战狼2吃奶昔'，单个单词都符合人类造词标准，放一起算出来的概率也很大，然而京巴和机器人都吃不了奶昔，只能说符合语法但是没有实际意义。\n",
    "\n",
    "如何提升：\n",
    "1.可以根据不同的语法采用不同的样本\n",
    "2.目前采用的是二元语法，也许三元或者以上的语法求解会比较正确。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 以下内容为可选部分，对于绝大多数同学，能完成以上的项目已经很优秀了，下边的内容如果你还有精力可以试试，但不是必须的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. (Optional) 完成基于Pattern Match的语句问答\n",
    "> 我们的GitHub仓库中，有一个assignment-01-optional-pattern-match，这个难度较大，感兴趣的同学可以挑战一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5. (Optional) 完成阿兰图灵机器智能原始论文的阅读\n",
    "1. 请阅读阿兰图灵关于机器智能的原始论文：https://github.com/Computing-Intelligence/References/blob/master/AI%20%26%20Machine%20Learning/Computer%20Machinery%20and%20Intelligence.pdf \n",
    "2. 并按照GitHub仓库中的论文阅读模板，填写完毕后发送给我: mqgao@kaikeba.com 谢谢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各位同学，我们已经完成了自己的第一个AI模型，大家对人工智能可能已经有了一些感觉，人工智能的核心就是，我们如何设计一个模型、程序，在外部的输入变化的时候，我们的程序不变，依然能够解决问题。人工智能是一个很大的领域，目前大家所熟知的深度学习只是其中一小部分，之后也肯定会有更多的方法提出来，但是大家知道人工智能的目标，就知道了之后进步的方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，希望大家对AI不要有恐惧感，这个并不难，大家加油！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1561828422005&di=48d19c16afb6acc9180183a6116088ac&imgtype=0&src=http%3A%2F%2Fb-ssl.duitang.com%2Fuploads%2Fitem%2F201807%2F28%2F20180728150843_BECNF.thumb.224_0.jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
